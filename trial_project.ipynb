{"cells":[{"cell_type":"code","execution_count":null,"id":"a6d7f70e","metadata":{"id":"a6d7f70e"},"outputs":[],"source":["!pip install datasets transformers evaluate"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"KYcZWIpk_cHh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673791340566,"user_tz":360,"elapsed":27997,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"2022a6da-52cf-4362-d1f9-2746e32deb64"},"id":"KYcZWIpk_cHh","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# This line can be adjusted so long as the working directory contains\n","# the data files found in \"chai_trial_project\"\n","cd /content/drive/MyDrive/UChicago/Career Advancement/CHAI Lab/Application/chai_trial_project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxF2Lvu5uouJ","executionInfo":{"status":"ok","timestamp":1673791351611,"user_tz":360,"elapsed":2942,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"61a7564a-e116-4e03-81bc-318edbbbff44"},"id":"LxF2Lvu5uouJ","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/UChicago/Career Advancement/CHAI Lab/Application/chai_trial_project\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from datasets import load_dataset, ClassLabel, concatenate_datasets\n","from transformers import AutoTokenizer, DataCollatorWithPadding, \\\n","AutoModelForSequenceClassification, TrainingArguments, Trainer\n","import evaluate\n","import matplotlib.pyplot as plt\n","from pathlib import Path"],"metadata":{"id":"dVTkc547_R4K","executionInfo":{"status":"ok","timestamp":1673791401740,"user_tz":360,"elapsed":8093,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}}},"id":"dVTkc547_R4K","execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"id":"c4b4a6b9","metadata":{"id":"c4b4a6b9","executionInfo":{"status":"ok","timestamp":1673791401740,"user_tz":360,"elapsed":5,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}}},"outputs":[],"source":["data_files = {\n","    'train': './op_task/train_op_data.jsonlist.bz2', \n","    'test': './op_task/heldout_op_data.jsonlist.bz2'\n","}"]},{"cell_type":"code","execution_count":6,"id":"cfa10010","metadata":{"id":"cfa10010","colab":{"base_uri":"https://localhost:8080/","height":187,"referenced_widgets":["c5c3c35dccb948f3a0d24086c416956f","d827d8a32bc24e5b895000b607379428","4050f4df5ec5498483c9dfaf3dfa624a","84fbb76d1f2c428e9ce2146811db68cf","41a0e55709694fcebe503cb6ce48a11e","e8ce79adaf91451fbd7633c1fb737758","87fb45258d8942aa80da8766f45b1be8","aac4c34ec4314d839cb58891ca0d22a8","ec6bc2e3123346bd96a537cdbc58a354","e560d0c83e3a4a9cbba138c5f5de7eb0","01581332ec1b42a69c07e75f7d5aca59","c72dfac897494f779f6153559fdda098","92592171fb714c04848f18579f2faf59","81c4bc00028f499595dfc82df4a23775","db209905e74743e9b05ac37fcbf34009","ac06f40dbfbc4817abe5fe77180bff2e","474d98c06d7b43f484d78bac2c38afa2","ac6b2d55a3034e17aba39d17bf327d5f","e625101bc9dc48d6a4e349be4f32e951","b30219eb843448db802d5cd8d8297745","dd8181132f234d8ebb1796edaf7288f0","0091d023c9864b16853a6cce9a5904a9","e0e40f2e12cb4a5e854b1ed365ea674e","e1a9dd10b61d4adfae7f4b4430ffe999","a412d4e8bcc348428ffbaac310f264aa","aab5bec01df944a99395992f7eb6c558","6f537105999349f0912a8d9ab2506872","4623517890a544fb96c9101a919e3f5a","ea6e7a3700db440aaf5ee46ed2a40c86","10f28d4692ac4401889e35dbf5e04715","dbfe662121004861bcd59fab251fced9","5dd4083f4e5f4754a4f43bf99f030902","8d7591b2cb5f4d679e3e5f886908cfa5","241b3dfa48204ecb9abc7ae47c95b0b6","d1426a492c5d4068936a896c2584f548","7cb87532d78148b2a05a0c1b84294e2a","434d1b9617bd4f758e01713b8833165a","b1dea5ad610b4f6f9987f74141ffa6d2","f7e4eb8e2c194ffdb63a90b7dcb72331","02a5c45eb2df4bcfb00904cc86eeb056","30ab6dfde96f4e7192d5e0b162b5186f","1c61469dac4245b8a384e7f0d12c2294","533fde3de54a402792415fc30f5a87bf","ef068d7bc0e948e99ed27e042ee5b75a","e4e4641486d84e88bdb6a5cb8f05f437","75d2d00b4b42421298f9869860e4775c","418493b907de4c3fb1d76611ae4f5ef3","1456f7f8dbbe43739325b921d5aae85b","c98bccc58b1e436bbd0dee4fb896eaac","e9d134ce3f7c4cbb89083f476eb2cde4","eb0a7f501b024fb5aae387410b5d6d40","f8251520eefc46d582bb750f3bfd1ddf","865eb6660143493fb35063ee446da121","4ad4b96cf0c848828c9c9c2ec982f02b","960287aad97a4431bfa51102cdd14c72"]},"executionInfo":{"status":"ok","timestamp":1673791407211,"user_tz":360,"elapsed":5475,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"16bb3cec-d05a-4cfa-9af3-715dba36e92f"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Using custom data configuration default-c16f624fcb6ff284\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-c16f624fcb6ff284/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c3c35dccb948f3a0d24086c416956f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c72dfac897494f779f6153559fdda098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0e40f2e12cb4a5e854b1ed365ea674e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"241b3dfa48204ecb9abc7ae47c95b0b6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c16f624fcb6ff284/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4e4641486d84e88bdb6a5cb8f05f437"}},"metadata":{}}],"source":["persuade_data = load_dataset('json', data_files=data_files)"]},{"cell_type":"code","source":["# Data exploration\n","persuade_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UT_qPe7j6kVr","executionInfo":{"status":"ok","timestamp":1673791407212,"user_tz":360,"elapsed":24,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"8437c67f-03ec-45c8-a7d6-da6361261f5e"},"id":"UT_qPe7j6kVr","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['title', 'delta_label', 'name', 'selftext'],\n","        num_rows: 10743\n","    })\n","    test: Dataset({\n","        features: ['title', 'delta_label', 'name', 'selftext'],\n","        num_rows: 1529\n","    })\n","})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":8,"id":"c4711d88","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c4711d88","executionInfo":{"status":"ok","timestamp":1673791407212,"user_tz":360,"elapsed":21,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"5e3f82bc-e4f3-4c5b-8781-e9dc40b46864"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': 'CMV: Iran has the right to develop nuclear weapons',\n"," 'delta_label': False,\n"," 'name': 't3_2rpfn7',\n"," 'selftext': \"First off, I do not believe that Iran *should* have nuclear weapons. In fact, I believe Iran having nuclear weapons makes the world less safe overall. However, I believe that as a sovereign nation they have the right to develop nuclear weapons if they so choose.\\n\\nWhy do I believe this:\\n\\n1. It is in Iran's best strategic interests to develop nuclear weapons in order to counter Israel (which has nuclear weapons), and additionally to one-up Saudi Arabia (their main regional rival), and guarantee their safety against other Arab nations with whom they have historically had rocky relations (Iraq, for example).\\n2. If Israel can illegally possess nuclear weapons (they haven't officially acknowledge they have nuclear weapons, nor have they signed the UN's non-proliferation treaty, making the weapons they do possess illegal), why can't Iran possess nuclear weapons, other than Western bias? Pakistan has nuclear weapons, and they are officially an Islamic Republic, and they can have nuclear weapons, even though their nuclear weapons are probably much more of a global security risk. \\n\\n3. Iran has proven itself to be a pragmatic and rational actor in world affairs. There is no reason to suspect that they would actually use a nuclear weapon, nor sell it to a terrorist group who would. They mainly want a nuclear weapon to secure their military position in the region, stick it to their rivals, and as a point of national pride. Because the West says Iran can't have a nuclear weapon, they want one all the more, and won't back down on that because of national pride, something an American should be able to sympathize with. \\n\\nNow, I recognize that Iran should not have nuclear weapons because it would cause an arms race in the most volatile region on Earth, but that doesn't mean that they, as a sovereign nation, do not have the right to develop a nuclear weapon. \\n\\n_____\\n\\n&gt; *Hello, users of CMV! This is a footnote from your moderators. We'd just like to remind you of a couple of things. Firstly, please remember to* ***[read through our rules](http://www.reddit.com/r/changemyview/wiki/rules)***. *If you see a comment that has broken one, it is more effective to report it than downvote it. Speaking of which,* ***[downvotes don't change views](http://www.reddit.com/r/changemyview/wiki/guidelines#wiki_upvoting.2Fdownvoting)****! If you are thinking about submitting a CMV yourself, please have a look through our* ***[popular topics wiki](http://www.reddit.com/r/changemyview/wiki/populartopics)*** *first. Any questions or concerns? Feel free to* ***[message us](http://www.reddit.com/message/compose?to=/r/changemyview)***. *Happy CMVing!*\"}"]},"metadata":{},"execution_count":8}],"source":["persuade_data['train'][1]"]},{"cell_type":"code","execution_count":9,"id":"9526cf8d","metadata":{"id":"9526cf8d","executionInfo":{"status":"ok","timestamp":1673791407213,"user_tz":360,"elapsed":16,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}}},"outputs":[],"source":["# Cleans up the text section of a CMV post. The code is\n","# borrowed from https://vene.ro/blog/winning-arguments-attitude-change-reddit-cmv.html\n","def cleanup(selftext):\n","    lines = [line for line in selftext.splitlines()\n","             if not line.lstrip().startswith(\"&gt;\")\n","             and not line.lstrip().startswith(\"____\")\n","             and \"edit\" not in \" \".join(line.lower().split()[:2])\n","            ]\n","    return \"\\n\".join(lines)\n","\n","def clean_selftext(example):\n","    return {'selftext': cleanup(example['selftext'])}"]},{"cell_type":"code","execution_count":10,"id":"7fce2f11","metadata":{"id":"7fce2f11","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["e706495f91df4ddb96aab4fa0133ccee","7fb73adcadef4f4c8433fcf519b18c4d","63c6caf3fa9f4922a195012d4202e54c","9486388c9948426eaf36743a0fa1bd3e","8c3ad4d77a8b465a8e0d64f5e2fa99c3","16c8b294ab8544a6b9de1542600da123","e02e75473c4345a5aedaee6253af7a96","90ad710a55fc4abbb5e1a75aa1c6490c","316a22ed58754d30a6b6465f34b1ab83","7da06fca7e5e4c86be91199265eedd93","85d6ef1251bf4e4384b480e561de944d","c3214f9e5f254e55a26f41d4a5c176af","7139bd6597134e93a663778fa9b1f060","fe163c4a2141400fbf0bf7bd840e53b1","f7ff04c58cc142269c6557236cd10a8b","d98fb60651794af08f55b13f07b43c20","d0871162ec734e6a8b05642d2700a424","89f1d668b5b04ef6b73bd1698e3baec8","3ed2e6652faf49cfa5ef8a7272ed509b","ad520c0204e64adbaccf6ba302f0f68a","cf6bb00090e6424fad31f412c670b54a","b542f48b845a452483f7daf574550dc6"]},"executionInfo":{"status":"ok","timestamp":1673791408409,"user_tz":360,"elapsed":1211,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"0e085966-d973-4520-efdd-6bcb4beffd13"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10743 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e706495f91df4ddb96aab4fa0133ccee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1529 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3214f9e5f254e55a26f41d4a5c176af"}},"metadata":{}}],"source":["persuade_data = persuade_data.map(clean_selftext)"]},{"cell_type":"code","execution_count":11,"id":"51fa8f3c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51fa8f3c","executionInfo":{"status":"ok","timestamp":1673791408409,"user_tz":360,"elapsed":5,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"5f19e06d-531f-4fd8-ba1f-05cedd51d43c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': 'CMV: Iran has the right to develop nuclear weapons',\n"," 'delta_label': False,\n"," 'name': 't3_2rpfn7',\n"," 'selftext': \"First off, I do not believe that Iran *should* have nuclear weapons. In fact, I believe Iran having nuclear weapons makes the world less safe overall. However, I believe that as a sovereign nation they have the right to develop nuclear weapons if they so choose.\\n\\nWhy do I believe this:\\n\\n1. It is in Iran's best strategic interests to develop nuclear weapons in order to counter Israel (which has nuclear weapons), and additionally to one-up Saudi Arabia (their main regional rival), and guarantee their safety against other Arab nations with whom they have historically had rocky relations (Iraq, for example).\\n2. If Israel can illegally possess nuclear weapons (they haven't officially acknowledge they have nuclear weapons, nor have they signed the UN's non-proliferation treaty, making the weapons they do possess illegal), why can't Iran possess nuclear weapons, other than Western bias? Pakistan has nuclear weapons, and they are officially an Islamic Republic, and they can have nuclear weapons, even though their nuclear weapons are probably much more of a global security risk. \\n\\n3. Iran has proven itself to be a pragmatic and rational actor in world affairs. There is no reason to suspect that they would actually use a nuclear weapon, nor sell it to a terrorist group who would. They mainly want a nuclear weapon to secure their military position in the region, stick it to their rivals, and as a point of national pride. Because the West says Iran can't have a nuclear weapon, they want one all the more, and won't back down on that because of national pride, something an American should be able to sympathize with. \\n\\nNow, I recognize that Iran should not have nuclear weapons because it would cause an arms race in the most volatile region on Earth, but that doesn't mean that they, as a sovereign nation, do not have the right to develop a nuclear weapon. \\n\\n\"}"]},"metadata":{},"execution_count":11}],"source":["persuade_data['train'][1]"]},{"cell_type":"code","execution_count":26,"id":"83d18dc1","metadata":{"id":"83d18dc1","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["e037ad334f6840918a3c4bb6d65d778b","eb922104f6c14908ad4aba86271acfe9","38b131715e6443288fafa5c6394bd399","716d64fb303c415db80cf3bdcb1728f1","96c46fff73894bc2ae79b2ddcd0a199a","74e42292cacb4ae2b61ce3f1587cc21d","5f6b621214214721b9b412ec18d61f71","961a6b41e7ae467a813244742c702255","b1ee6faec1ec480e960fb3c4187f660a","3bb93691e8824e328ac4f4c273160631","1a52ff08976443bb989d2e0366b6fb9b","d6d86c795c9d4bc5bc5d87e977795d89","c3413b3d3d7444549399a76eb222adf8","c7e4a396a9204ee1956611cef0cf487c","0fdf303a37f24233bbad02f342da034f","6e9a2da1bada40279a46f33513bedc42","b37a6f0abfd54cf896943bc884d37adc","228501d5a6254bbca784fea68932c09b","023051eadca744c893c8664b77d37b3d","1ac5c84aab72444b9ac2859cf837da28","4acb09d9bca642feb77d48a7b45e73e9","7a63269df8ca49b7831f50814a8b1a9c","e92039a4e0c14b839c2bc1741cb46311","28e3c967016b4a8e931de6b4ecf87f59","e874010ee33c4c2b9a5fbd2d71347e86","a145262f4566466d8e3e2cdb0c3d0d1c","4f20417b73dd4a4e8277a256d373e04b","c5f1e2e6e0f14f7e9d647d135566ec38","7855cbfac1e947c5bbe7a701a4234bbd","df85577f0d814c47b8fa88d7f880f970","f0674979ef3c46ec813b9d41fafd2ef6","c7f5b7621b3c4d04b884290e8d1864d1","61e5ddba635b44f09ee16cf803668f90","900f70509dfd447e9b04de22e69d2012","7b87aceda68d4d43b1c8063188b15e53","9f308c6c7ea04560affa7de80f6bc6f2","7738b4ffde9040c48ac229e7740bc9b8","5453797a2d114e18b9aab6e8a17e6d61","5d3d50c715ea4f7eb8f11a96d857a92c","8255b87c99094226b3998a83e58406c7","15aaebb7d4ce4faa8055a562057e286a","bb145261594b4b19abccda54cfc6c735","71f4207c750b44709072e4511ad1206a","cced0ee065014877a894a438995491d6"]},"executionInfo":{"status":"ok","timestamp":1673792335991,"user_tz":360,"elapsed":11591,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"1ba7cdd3-5abd-41ce-b99a-48332a71b97c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e037ad334f6840918a3c4bb6d65d778b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d86c795c9d4bc5bc5d87e977795d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e92039a4e0c14b839c2bc1741cb46311"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"900f70509dfd447e9b04de22e69d2012"}},"metadata":{}}],"source":["# All of the models used in experiments\n","\n","checkpoint = 'bert-base-uncased'\n","# checkpoint = 'bert-base-cased'\n","# checkpoint = 'distilbert-base-uncased'\n","# checkpoint = 'distilbert-base-cased'\n","# checkpoint = 'allenai/longformer-base-4096'\n","\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":27,"id":"d9c84e56","metadata":{"id":"d9c84e56","executionInfo":{"status":"ok","timestamp":1673792335992,"user_tz":360,"elapsed":20,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}}},"outputs":[],"source":["# A function that tokenizes the examples' selftexts\n","# that can be mapped over the data splits\n","def tokenize_func(example):\n","    return tokenizer(example['selftext'], truncation=True)"]},{"cell_type":"code","execution_count":28,"id":"0b1a360b","metadata":{"id":"0b1a360b","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["30eac4e487b844f482f1271f91225dec","36c358debeb543128caa4e8deecb3a1b","9c404c3802e248a2ae55adf406bb57d0","0fceb9f8bd8f454b925f925e5da68383","bea74e72cc4b4dcb9365b9745b8958b8","c7f6ac69a6e64f94b1613fa747bb77c6","0d35ce99d9f24a85b234d16c67c06351","ceaa046af24f4526a4ebce5eb3195f48","cc5f5515c9a544c39a2fdcf2c5b209b9","cb16f1fb0c92479dbecf046f01eee9f9","67a13faf22244f9ea7f03cd94902a4ea","d65e53a90047456fb6c0b7314f1afa96","520b500188af44f283c0588fcf2a8104","5fedee0a5f424d80b7e878ae7651d02e","3b09e32c3f7b45eeb12c0a61d04e49bc","7e44d8968ee145d28c74260bcca447d6","fd727d1f09d842d4b191a0e3e94c6e5c","4e53fafb50df4166b23b11a4133ef8c5","0fe00d2b59594d0d9809b9125a757b59","1286f6c312dc40899bd55bddef5ff908","280cee7177114ef3b037e5d0277027a0","1cde6d77f5a24be4b23b157237d34682"]},"executionInfo":{"status":"ok","timestamp":1673792359436,"user_tz":360,"elapsed":23463,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"cb8e48d8-9214-483a-e20c-49a73c8439be"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10743 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30eac4e487b844f482f1271f91225dec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1529 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d65e53a90047456fb6c0b7314f1afa96"}},"metadata":{}}],"source":["# Tokenizes the data\n","token_data = persuade_data.map(tokenize_func)"]},{"cell_type":"code","execution_count":29,"id":"2e0707de","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2e0707de","executionInfo":{"status":"ok","timestamp":1673792359437,"user_tz":360,"elapsed":14,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"872e4a1f-1250-424f-caf8-c6b769e4a208"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['title', 'delta_label', 'name', 'selftext', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 10743\n","    })\n","    test: Dataset({\n","        features: ['title', 'delta_label', 'name', 'selftext', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 1529\n","    })\n","})"]},"metadata":{},"execution_count":29}],"source":["token_data"]},{"cell_type":"code","source":["# Checking the lengths of processed samples. The lengths will be truncated\n","# for BERT-based models and remain unchanged for the Longformer.\n","token_data = token_data.map(lambda x: {'length': len(x['input_ids'])})"],"metadata":{"id":"hDFii_vliBiy","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["5dccd037611b448cb86647738e369610","35727692b797415aab04f31d8507f120","c81ef4dc00d64d94b3a76b24632391f4","ad3ac590dcc84f0eb9b4172082d4e36c","627a0d98a4814a87b3587880fb4c5678","258fd565b7fe4b43baf9285db59708e8","b06feac693ef4a24820dd76f94955f5a","c4168406cd7449e28bc2df81f254e0fe","f1397f5ee7bd407c995394450c7f9d76","9466f5a718a24cbaa6856fcefbecc284","8983a28a572c4369a8e9ee1d77ad18bd","c33ee3b8d0de4f76a22106de3a54fc8e","20df600bcbb14aa3a12bfd5f3e3f47f1","7dd725a339b94918a4ca12753de0585d","875e3ffe72114f38a1660994769b680f","161965332bbc4ef9b20e65ec9d8bbd20","5694d97709db4e9d88a9c06c372cf695","c56d77c8064948bf87bfe49a358962d7","4ecacdb6501444c798aadbb455e18e39","b6e6f218c39843b4b326165454f2c5bc","a49537a2a36f4c1e832a6c97e66f218b","df2eef555fa14bda91129bdc1b34b8b9"]},"executionInfo":{"status":"ok","timestamp":1673792366418,"user_tz":360,"elapsed":6991,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"9631221f-00e3-4ad1-f790-d5ebe00de8e7"},"id":"hDFii_vliBiy","execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10743 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dccd037611b448cb86647738e369610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1529 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33ee3b8d0de4f76a22106de3a54fc8e"}},"metadata":{}}]},{"cell_type":"code","source":["data_lengths = token_data['train']['length']"],"metadata":{"id":"usNcOOS0iB7o","executionInfo":{"status":"ok","timestamp":1673792366419,"user_tz":360,"elapsed":21,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}}},"id":"usNcOOS0iB7o","execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Possible text lengths under BERT-based models\n","Path('./plots/').mkdir(parents=True, exist_ok=True)\n","\n","fig, ax = plt.subplots()\n","\n","num_pts = len(data_lengths)\n","wts = [1 / num_pts for length in data_lengths]\n","ax.hist(data_lengths, weights=wts)\n","\n","ax.set_xlabel('Number of Tokens', fontsize=12)\n","ax.set_ylabel('Proportion', fontsize=12)\n","fig.savefig('./plots/bert_tokens.png', bbox_inches='tight')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"joEtrtE1iBlJ","executionInfo":{"status":"ok","timestamp":1673792366740,"user_tz":360,"elapsed":340,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"c11ae812-3179-40bf-9332-c949b5f1e8c0"},"id":"joEtrtE1iBlJ","execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# Possible text lengths under Longformer\n","Path('./plots/').mkdir(parents=True, exist_ok=True)\n","\n","fig, ax = plt.subplots()\n","\n","num_pts = len(data_lengths)\n","wts = [1 / num_pts for length in data_lengths]\n","ax.hist(data_lengths, weights=wts)\n","\n","ax.set_xlabel('Number of Tokens', fontsize=12)\n","ax.set_ylabel('Proportion', fontsize=12)\n","fig.savefig('./plots/longformer_tokens.png', bbox_inches='tight')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":285},"id":"uDUQGzlbivlq","executionInfo":{"status":"ok","timestamp":1673792228135,"user_tz":360,"elapsed":639,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"780952b6-fab5-4d3b-b930-255233f0f634"},"id":"uDUQGzlbivlq","execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEMCAYAAAArnKpYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZDUlEQVR4nO3dfbRddX3n8ffHIFDBKsitBRJI0FibTl1qI/g01ic0gELXWGtQR1A0tR3U1oqNdQYVq0VcS61LRpsiPhYjpc40hSiiIk59aqIgNVDgErEJqEQefFYIfOePvbNzuNyQe5J797lJ3q+1zsrev/M7Z3/Pb62cz90P57dTVUiSBHC/URcgSZo9DAVJUsdQkCR1DAVJUsdQkCR1DAVJUqe3UEiyJMk1ScaTLJ/k+cOSXJrk8iRXJjm2r9okSY308TuFJHOAa4GjgY3AGuDEqrpqoM8K4PKqen+SRcDqqpo/48VJkjp79bSdI4HxqloPkGQlcAJw1UCfAn69XX4QcNP23vSggw6q+fPnT2+lkrSb+8Y3vvHDqhqb7Lm+QuFQYMPA+kbgqAl93gx8NsmrgP2AZ072RkmWAcsADjvsMNauXTvtxUrS7izJd7f13Gw60Xwi8OGqmgscC3wsyb3qq6oVVbW4qhaPjU0adJKkHdRXKNwIzBtYn9u2DToFOB+gqr4K7Asc1Et1kiSgv1BYAyxMsiDJ3sBSYNWEPv8JPAMgyW/ThMKmnuqTJNFTKFTVZuBU4GLgauD8qlqX5Iwkx7fd/gJ4RZJvAZ8ATi6ncJWkXvV1opmqWg2sntB2+sDyVcCT+qpHknRvs+lEsyRpxAwFSVLHUJAkdQwFSVKntxPNs8385ReNbNs3nHncyLYtSffFPQVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUqe3UEiyJMk1ScaTLJ/k+XcnuaJ9XJvk9r5qkyQ1epk6O8kc4GzgaGAjsCbJqva+zABU1Z8P9H8V8Jg+apMkbdXXnsKRwHhVra+qO4CVwAn30f9E4BO9VCZJ6vQVCocCGwbWN7Zt95LkcGAB8IVtPL8sydokazdt2jTthUrSnmw2nmheClxQVXdN9mRVraiqxVW1eGxsrOfSJGn31lco3AjMG1if27ZNZikeOpKkkegrFNYAC5MsSLI3zRf/qomdkjwSOAD4ak91SZIG9BIKVbUZOBW4GLgaOL+q1iU5I8nxA12XAiurqvqoS5J0T71ckgpQVauB1RPaTp+w/ua+6pEk3dtsPNEsSRoRQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1OktFJIsSXJNkvEky7fR54+SXJVkXZLz+qpNktTo5R7NSeYAZwNHAxuBNUlWVdVVA30WAm8AnlRVtyX5jT5qkyRt1deewpHAeFWtr6o7gJXACRP6vAI4u6puA6iqm3uqTZLU6isUDgU2DKxvbNsGPQJ4RJIvJ/lakiWTvVGSZUnWJlm7adOmGSpXkvZMs+lE817AQuCpwInA3yd58MROVbWiqhZX1eKxsbGeS5Sk3VtfoXAjMG9gfW7bNmgjsKqq7qyq7wDX0oSEJKknfYXCGmBhkgVJ9gaWAqsm9Pm/NHsJJDmI5nDS+p7qkyTRUyhU1WbgVOBi4Grg/Kpal+SMJMe33S4GbklyFXApcFpV3dJHfZKkRi+XpAJU1Wpg9YS20weWC3ht+5AkjcBsOtEsSRoxQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1OktFJIsSXJNkvEkyyd5/uQkm5Jc0T5e3ldtkqRGL/doTjIHOBs4GtgIrEmyqqqumtD1k1V1ah81SZLura89hSOB8apaX1V3ACuBE3ratiRpivoKhUOBDQPrG9u2iZ6X5MokFySZN9kbJVmWZG2StZs2bZqJWiVpjzXlw0dJDgReBzwa2H/wuap6yjTU8i/AJ6rqV0n+GPgI8PSJnapqBbACYPHixTUN25UktYY5p3AesA9wPvDzIbdzIzD4l//ctq1TVbcMrJ4DnDXkNiRJO2mYUHgiMFZVv9qB7awBFiZZQBMGS4EXDnZIcnBVfa9dPR64ege2I0naCcOEwpU0f+FfP+xGqmpzklOBi4E5wLlVtS7JGcDaqloFvDrJ8cBm4Fbg5GG3I0naOcOEwheAzyT5EPD9wSeq6tztvbiqVgOrJ7SdPrD8BuANQ9QjSZpmw4TCf6W5aujoCe0FbDcUJEmz35RDoaqeNpOFSJJGb6hfNCc5AHguzW8MbgT+papum4nCJEn9m/KP15I8geYk8yuBRwF/DFzftkuSdgPD7Cm8B/jTqlq5pSHJC4D3Ao+b7sIkSf0bZpqLR9D8cG3QBcDDp68cSdIoDRMK19H86GzQ89mB3y1IkmanYQ4f/RlwYZJXA98F5gMLgefMQF2SpBEY5pLUryR5GHAccAjNBHarq+rWmSpOktSvoS5JbS8//fgM1SJJGrH7DIUkn6mqJe3y/6P59fK9TNPU2ZKkEdvensJHB5bPmclCJEmjd5+hUFXnDaz+R1V9fWKfJEdOe1WSpJEY5pLUS7bR/pnpKESSNHrbPdGc5H5AmsWkXd7iYTT3P5Ak7QamcvXRZpoTzOHeAXA38LbpLkqSNBpTCYUFNIFwGTB4lVEBm6rqFzNRmCSpf9sNhar6bpI5wHeA7+/gPZolSbuAKZ1orqq7aPYYhjkxfQ9JliS5Jsl4kuX30e95SSrJ4h3dliRpxwzzJf8W4P1JDk8yJ8n9tjy298J2T+Ns4BhgEXBikkWT9Hsg8BrgXpe+SpJm3jChcA7wEmA9cAdwJ82J5zun8NojgfGqWl9VdwArgRMm6fdW4B3AL4eoS5I0TYaZ+2jBTmznUGDDwPpG4KjBDkkeC8yrqouSnLYT25Ik7aBhZkn9LnS/W3go8IOquns6imjf813AyVPouwxYBnDYYYdNx+YlSa1h7tH860k+SnNo50bgF0k+kuRBU3j5jcC8gfW5bdsWDwT+C/DFJDcAjwdWTXayuapWVNXiqlo8NjY21fIlSVMwzDmF9wL70Xx5/xrwu8AD2vbtWQMsTLIgyd40d3BbteXJqvpRVR1UVfOraj7wNeD4qlo7RH2SpJ00zDmFJcARVfXzdv3aJC9lCrfjrKrNSU4FLgbmAOdW1bokZwBrq2rVfb+DJKkPw4TCL4ExmltxbnEQMKUfs1XVamD1hLbTt9H3qUPUJUmaJsOEwjnAJUneRRMMhwN/DqyYicIkSf0bJhTeBtwEvJDmHs03AWcB585AXZKkERjmktSiCQBDQJJ2U0PNZZTkZUkuSbKu/feU9h4LkqTdwJT3FJKcRTM1xXvYek7hdcBvAa+fkeokSb0a5pzCycBjq2rjloYkFwLfxFCQpN3CMIePftI+Jrb9ePrKkSSN0jB7Cu8BPpXkTJoJ7eYBpwHvTnLElk5VtX56S5Qk9WWYUPjb9t+nTWh/BlunuiiaXyxLknZBw1ySusN3XZMk7RqG2VMAIMlhNPdH2FhVG7bXX5K06xhm6uyDk1wGjAOfAq5P8qUkh8xYdZKkXg1zSOj9wLeAA6rqYOAA4HLgAzNRmCSpf8McPnoycHBV3QlQVT9L8nruebMcSdIubJg9hduARRPafgu4ffrKkSSN0jB7CmcBn0vyQbZOc/FS4H/NRGGSpP4Nc0nq3ye5nmbq7EfRTqNdVZ+fqeIkSf2aUigkmQNcCyyqqi/MbEmSpFGZ0jmFqroLuAvYd2bLkSSN0jAnmt8DnJ/k95M8LMkRWx5TeXGSJUmuSTKeZPkkz78yyb8nuSLJvyaZeFJbkjTDhjnR/L7236MntG93vqP28NPZ7Ws3AmuSrKqqqwa6nVdVH2j7Hw+8C1gyRH2SpJ203T2FJA9I8nbgIuCtwAOq6n4Dj6lMgHckMF5V66vqDmAlzQ17OlU1OAX3fjRhI0nq0VT2FM4GFgOfBp4HHAi8asjtHAoMzpO0EThqYqck/wN4LbA38PTJ3ijJMmAZwGGHHTZkGZKk+zKVcwpLgGdV1euBY4DnzFQxVXV2VT0M+Evgf26jz4qqWlxVi8fGxmaqFEnaI00lFParqu8BtLOiPmgHtnMjzU15tpjLfU+PsRL4gx3YjiRpJ0zl8NFeSZ4GZBvrTOG3C2uAhUkW0ITBUpofwXWSLKyq69rV44DrkCT1aiqhcDNw7sD6LRPWC7jPy1KranOSU4GLaa5UOreq1iU5A1hbVauAU5M8E7iTZp6lk6b+MSRJ02G7oVBV86djQ1W1Glg9oe30geXXTMd2JEk7zltsSpI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqdNbKCRZkuSaJONJlk/y/GuTXJXkyiSfT3J4X7VJkhq9hEKSOcDZwDHAIuDEJIsmdLscWFxVjwIuAM7qozZJ0lZ97SkcCYxX1fqqugNYCZww2KGqLq2qn7erXwPm9lSbJKnVVygcCmwYWN/Ytm3LKcCnJ3siybIka5Os3bRp0zSWKEmadSeak7wYWAy8c7Lnq2pFVS2uqsVjY2P9FidJu7m9etrOjcC8gfW5bds9JHkm8Ebg96vqVz3VJklq9RUKa4CFSRbQhMFS4IWDHZI8Bvg7YElV3dxTXSMxf/lFI9nuDWceN5LtStp19HL4qKo2A6cCFwNXA+dX1bokZyQ5vu32TmB/4B+TXJFkVR+1SZK26mtPgapaDaye0Hb6wPIz+6pFkjS5WXeiWZI0OoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKnTWygkWZLkmiTjSZZP8vxTknwzyeYkf9hXXZKkrXoJhSRzgLOBY4BFwIlJFk3o9p/AycB5fdQkSbq3vXrazpHAeFWtB0iyEjgBuGpLh6q6oX3u7p5qkiRN0Nfho0OBDQPrG9u2oSVZlmRtkrWbNm2aluIkSY1d7kRzVa2oqsVVtXhsbGzU5UjSbqWvULgRmDewPrdtkyTNIn2FwhpgYZIFSfYGlgKretq2JGmKegmFqtoMnApcDFwNnF9V65KckeR4gCSPS7IReD7wd0nW9VGbJGmrvq4+oqpWA6sntJ0+sLyG5rCSJGlEdrkTzZKkmWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqdPb3EcavfnLLxrZtm8487iRbVvS1LmnIEnqGAqSpI6hIEnqGAqSpI6hIEnqGAqSpE5vl6QmWQL8LTAHOKeqzpzw/D7AR4HfA24BXlBVN/RVn2bWqC6H9VJYaTi97CkkmQOcDRwDLAJOTLJoQrdTgNuq6uHAu4F39FGbJGmrvvYUjgTGq2o9QJKVwAnAVQN9TgDe3C5fALwvSaqqeqpRu6FR/mBvVNw70s7oKxQOBTYMrG8EjtpWn6ranORHwEOAHw52SrIMWNau/jTJNTtY00ET31uTcpymblaMVWb/PvasGKddxEyN1eHbemKXm+aiqlYAK3b2fZKsrarF01DSbs1xmjrHamocp6kbxVj1dfXRjcC8gfW5bdukfZLsBTyI5oSzJKknfYXCGmBhkgVJ9gaWAqsm9FkFnNQu/yHwBc8nSFK/ejl81J4jOBW4mOaS1HOral2SM4C1VbUK+CDwsSTjwK00wTGTdvoQ1B7CcZo6x2pqHKep632s4h/jkqQt/EWzJKljKEiSOntkKCRZkuSaJONJlo+6nr4lOTfJzUm+PdB2YJJLklzX/ntA254k723H6sokjx14zUlt/+uSnDTZtnZlSeYluTTJVUnWJXlN2+5YDUiyb5J/S/Ktdpze0rYvSPL1djw+2V5kQpJ92vXx9vn5A+/1hrb9miTPHs0nmllJ5iS5PMmF7frsGqeq2qMeNCe6rweOAPYGvgUsGnVdPY/BU4DHAt8eaDsLWN4uLwfe0S4fC3waCPB44Ott+4HA+vbfA9rlA0b92aZ5nA4GHtsuPxC4lmaaFsfqnuMUYP92+f7A19vPfz6wtG3/APAn7fKfAh9ol5cCn2yXF7X/H/cBFrT/T+eM+vPNwHi9FjgPuLBdn1XjtCfuKXRTblTVHcCWKTf2GFX1JZorvAadAHykXf4I8AcD7R+txteAByc5GHg2cElV3VpVtwGXAEtmvvr+VNX3quqb7fJPgKtpfnnvWA1oP+9P29X7t48Cnk4zZQ3ce5y2jN8FwDOSpG1fWVW/qqrvAOM0/193G0nmAscB57TrYZaN054YCpNNuXHoiGqZTR5aVd9rl78PPLRd3tZ47VHj2O66P4bmr2DHaoL2kMgVwM00oXc9cHtVbW67DH7me0xpA2yZ0ma3HyfgPcDrgbvb9Ycwy8ZpTwwFbUc1+6heq9xKsj/wT8CfVdWPB59zrBpVdVdVPZpmtoIjgUeOuKRZJ8lzgJur6hujruW+7ImhMJUpN/ZEP2gPddD+e3Pbvq3x2iPGMcn9aQLhH6rqU22zY7UNVXU7cCnwBJrDZ1t+IDv4mbc1pc3uPk5PAo5PcgPNYeun09xjZlaN054YClOZcmNPNDjNyEnAPw+0v6S9subxwI/aQycXA89KckB79c2z2rbdRnv89oPA1VX1roGnHKsBScaSPLhd/jXgaJrzL5fSTFkD9x6nyaa0WQUsba+6WQAsBP6tn08x86rqDVU1t6rm03zvfKGqXsRsG6dRn4kfxYPmKpFraY57vnHU9Yzg838C+B5wJ83xyFNojlV+HrgO+BxwYNs3NDdIuh74d2DxwPu8jOYk1zjw0lF/rhkYpyfTHBq6EriifRzrWN1rnB4FXN6O07eB09v2I2i+rMaBfwT2adv3bdfH2+ePGHivN7bjdw1wzKg/2wyO2VPZevXRrBonp7mQJHX2xMNHkqRtMBQkSR1DQZLUMRQkSR1DQZLUMRS0x0ny4SR/PaJtJ8mHktyWZFquLU/yxSQvn473kgwFjVySG9JM5b3fQNvLk3xxhGXNlCfT/LhrblXdYxKzJH+V5Kft45dJ7hpYXzeacrWnMRQ0W8wBXjPqIoaVZM6QLzkcuKGqfjbxiap6e1XtX1X7A68Evrplvap+ZzrqlbbHUNBs8U7gdVumSxiUZH6SGpgf5h6HTJKcnOTLSd6d5PYk65M8sW3f0O6FTLyxzUFpbpDzkySXJTl84L0f2T53a3sTkz8aeO7DSd6fZHWSnwFPm6TeQ5Ksal8/nuQVbfspNFMmP6H96/8tUx2c9vOsSfKj9t8nbqPfwWlu8HNau/74JF9px+VbSZ46YQzf2o7dT5J8NslB7XP7Jvl4klva165J8tDJtqndi6Gg2WIt8EXgdTv4+qNopll4CM0NTFYCjwMeDrwYeF872+kWLwLeChxEM33FPwC0h7Auad/jN2jmqPnfSRYNvPaFwNtobrzzr5PUspJm+pBDaOaseXuSp1fVB7nnHsCbpvLBkhwIXAS8t/187wIuSvKQCf0WAJcB76uqdyY5tH3dX9Pc4Od1wD8lGZvwWV7afta92Tr+J9FMwDav3eYrgV9MpV7t2gwFzSanA6+a8KU1Vd+pqg9V1V3AJ2m+zM6o5kYknwXuoAmILS6qqi9V1a9o5pF5QpJ5wHNoDu98qKo2V9XlNLOkPn/gtf9cVV+uqrur6peDRbTv8STgL6vql1V1Bc3ewUt24DNtcRxwXVV9rK3pE8B/AM8d6LOIZmK1N1XVirbtxcDqqlrd1noJTfgeO/C6D1XVtVX1C5o7gD26bb+TJgweXs202N+oCdOGa/dkKGjWqKpvAxfS3OJyWD8YWP5F+34T2wb3FLqblFRz17Bbaf6yPxw4qj1kcnuS22n2Kn5zstdO4hDg1mru1LbFd9m5m6Ac0r7HoInv+SKa6ZMvGGg7HHj+hM/yZJrbjG7x/YHln7N1jD5GM5PryiQ3JTkrzTTi2s0ZCppt3gS8gnt+4W05KfuAgbbBL+kd0c1H3x5WOhC4ieYL/7KqevDAY/+q+pOB197XLJI3AQcmeeBA22Hs3Hz3N9F8wQ+a+J5vBn4InDdw8nsD8LEJn2W/qjpzexusqjur6i1VtQh4Is0e1M7s7WgXYShoVqmqcZrDP68eaNtE8wX44jS3fXwZ8LCd3NSxSZ6c5p4abwW+VlUbaPZUHpHkvye5f/t4XJLfnmL9G4CvAH/Tnqx9FM3U5B/fiVpXtzW9MMleSV5Ac7jowoE+d9Ic4toP+GiS+7XbfG6SZ7fjtm+Sp6a5T/B9SvK0JL/bBsyP2/e/ezsv027AUNBsdAbNl9ugVwCn0dx56ndovnh3xnk0eyW3Ar9Hc/yd9rDPs2hOMN9Ec3jlHcA+Q7z3icD89vX/h+Y4/+d2tNCquoXmL/W/oPn8rweeU1U/nNDvDuC/0dwz+lyaID0B+CtgE82ew2lM7f/9b9IcivoxzQ1zLqM5pKTdnPdTkCR13FOQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlS5/8Dm3PqPxAsicQAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","execution_count":null,"id":"62f86c2f","metadata":{"id":"62f86c2f"},"outputs":[],"source":["# Processing the columns to be recognizable by the model\n","token_data = token_data.remove_columns(['title','name','selftext','length'])\n","token_data = token_data.rename_column('delta_label','labels')"]},{"cell_type":"code","source":["# Re-formatting the labels from scalar to 2-D vectors so that\n","# they are compatible with the model's output. This step is\n","# necessary to compute the loss\n","mall_label = ClassLabel(num_classes=2, names=['fixed', 'malleable'])\n","token_data = token_data.cast_column('labels', mall_label)"],"metadata":{"id":"paOBNvSctVau","executionInfo":{"status":"ok","timestamp":1673752156679,"user_tz":360,"elapsed":874,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["b90ccd6acab04aafb70f693303b208bc","6135d440b6fc4ce79916263cc7a2b7f7","d9d9ec7b6447479094357c422145acab","da0c28464edb41268590d5347973ccfe","5219fe29a6f041eead0a9ca00bcdaf2f","3a9a10d5c813470c87864998a4c8587b","3ca670cae80d4efdad1f555bdf4bd3c7","f2adcac6a12647819937e127f6dd02fb","cb2f915218864a159417e192aba8cdeb","d1294d7a0e0a40cbb09f65b5ef912612","c7de5c6ab18a4b50a79b47959e5a6bf3","e7852c768edb4b48b0119c0ce52a969f","ced68193f7e64b1c861aed8237e0e892","c36b5128ea704e0a81744016490c5159","d6144d4d11c94c41a23b7b3fc4f4b4fd","e20766cbf13c46f29d3029a8181c609d","6b2169b3c9424e4986b85b6308a86a8b","3017da81eabb4c8fb3bfc02f551ac685","93144c73ebdb45ebb9992c345583e40d","609d977705d2424d88c073bf3f040f4c","c8b7eef2edc04811b34d5591c1aa5216","10e88c968f454e2eae15a5580d7d0165"]},"outputId":"53574a04-c4a8-4ffd-d537-34e04dcb1305"},"id":"paOBNvSctVau","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Casting the dataset:   0%|          | 0/11 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b90ccd6acab04aafb70f693303b208bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Casting the dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7852c768edb4b48b0119c0ce52a969f"}},"metadata":{}}]},{"cell_type":"code","source":["token_data['train'].features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zh0cAJTvtVgP","executionInfo":{"status":"ok","timestamp":1673752156680,"user_tz":360,"elapsed":10,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"be6e960d-d181-4a4b-b970-57ce738bd03e"},"id":"zh0cAJTvtVgP","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'labels': ClassLabel(names=['fixed', 'malleable'], id=None),\n"," 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n"," 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# Fixing class imbalance for the train set by under-sampling\n","train_pos = token_data['train'].filter(lambda x: x['labels'] == 1)\n","train_neg = token_data['train'].filter(lambda x: x['labels'] == 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["1bc0a23451c745ebac0d9856b59d9133","52960526673b42578e6fa39e35de9517","88ec15aa87d74562a9581c6c0aeec475","4776a12eda6c4ec2a245c98350c316bb","9602c17df23d41c7be4b76e756177fd4","b3501724fb4e4e9cb1c168ec1a460c7e","8293f86fe90a4fa58ef052f6c6df4d56","484856bfdcd84205a0b8a5eeb2475416","093b953b19f544a7a8185584435628ac","78da710c6d394afe9dc52bb1dd343b10","642c90ce4db84f39abb16c092c3cee24","0f0a4bb3c0464fb7824d1a544532d792","3c7391e9a36c40e4876a9083bc84554b","b5474b1d26e44891b510bf3b5eb97806","e2523e8aaafc4ec8ad4e2317ad3ac873","8d820b5247824eb38cdb3d673f7a2437","516f3fc8e94e4a958e0c9afe19dd4a25","c8cbead78a7940c89492929b3aa032e6","6396c49c1b364ccebb7b95d2712d048d","1134ba360eb84d5e80deb43fbb922e2e","68c70d4cfa9c4f20a55eb9a37699d770","f6e576d5abaf4db2aa3f4220f34cccfa"]},"id":"l8I1z12B23vH","executionInfo":{"status":"ok","timestamp":1673752164552,"user_tz":360,"elapsed":7880,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"bda470e8-7974-4756-bfa0-18557b954346"},"id":"l8I1z12B23vH","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/11 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc0a23451c745ebac0d9856b59d9133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/11 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f0a4bb3c0464fb7824d1a544532d792"}},"metadata":{}}]},{"cell_type":"code","source":["train_neg = train_neg.select(range(3191))"],"metadata":{"id":"b2TtiZht23xQ"},"id":"b2TtiZht23xQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_balance = concatenate_datasets([train_pos, train_neg])\n","train_balance = train_balance.shuffle(seed=42)\n","token_data['train'] = train_balance"],"metadata":{"id":"cMQOOD2-7c_d"},"id":"cMQOOD2-7c_d","execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9vnS62C83tk","executionInfo":{"status":"ok","timestamp":1673752164555,"user_tz":360,"elapsed":37,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"6b39d392-cbb3-41ca-d05c-484302914ddc"},"id":"K9vnS62C83tk","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['labels', 'input_ids', 'attention_mask'],\n","        num_rows: 6382\n","    })\n","    test: Dataset({\n","        features: ['labels', 'input_ids', 'attention_mask'],\n","        num_rows: 1529\n","    })\n","})"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","execution_count":null,"id":"2eb30ddc","metadata":{"id":"2eb30ddc"},"outputs":[],"source":["# The data collator batches samples together\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"id":"32b72b09","metadata":{"id":"32b72b09"},"outputs":[],"source":["# Loading a new model from HuggingFace\n","# model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","\n","# Loading a finetuned model from a checkpoint\n","# model = AutoModelForSequenceClassification.from_pretrained('./models/distilbert-base-cased/checkpoint-4500')\n","model = AutoModelForSequenceClassification.from_pretrained('./models/allenai/longformer-base-4096/checkpoint-2000')"]},{"cell_type":"code","source":["# Freezing the BERT parameters to avoid destroying the pretrained weights\n","# for param in model.bert.parameters():\n","#   param.requires_grad = False\n","\n","# For DistilBERT\n","# for param in model.distilbert.parameters():\n","#   param.requires_grad = False\n","\n","# For Longformer\n","for param in model.longformer.parameters():\n","  param.requires_grad = False"],"metadata":{"id":"CQCFKtuHpXLZ"},"id":"CQCFKtuHpXLZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Freezing only the word embeddings\n","# for param in model.bert.embeddings.parameters():\n","#   param.requires_grad = False"],"metadata":{"id":"uImzHrDJpquI"},"id":"uImzHrDJpquI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(eval_preds):\n","    metric = evaluate.load('roc_auc')\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(references=labels, prediction_scores=predictions)"],"metadata":{"id":"pafqKcZ-EP1P"},"id":"pafqKcZ-EP1P","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_args = TrainingArguments('./models/' + checkpoint, evaluation_strategy='epoch', num_train_epochs=3, learning_rate=1e-06)\n","# train_args = TrainingArguments('./models/' + checkpoint + '-final', evaluation_strategy='epoch', num_train_epochs=3, learning_rate=1e-05)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xngQ8LQaEP4t","executionInfo":{"status":"ok","timestamp":1673755241674,"user_tz":360,"elapsed":597,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"1f985e57-6928-454a-ffde-b77a871bacf3"},"id":"xngQ8LQaEP4t","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","execution_count":null,"id":"84ea19d0","metadata":{"id":"84ea19d0"},"outputs":[],"source":["trainer = Trainer(\n","    model, \n","    train_args, \n","    train_dataset=token_data['train'],\n","    eval_dataset=token_data['test'],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","source":["# Training Longformer for 6 epochs (first 3 not shown). The best performance is \n","# achieved at 5 epochs, despite the losses' decrease on the 6th epoch\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["efa248f6236043c8be129d5c5c29e44d","0c4bcf750a1b49908bf285a323ea2734","56caba446452420c9264f4716a54db73","91ecadb081284d5b95e488d328148c92","8fd971371edf43b5bc9aa2db2290b045","2c31541a0b5840668bcbea9924626058","ef094b029e4a4413a81bd742a1187611","42f6939f28ae4eaab08fd69bfc64b9f1","8a59af6e781246c0a7f232bc4cfd6178","f20948455eee42bcae5e05ec2d24f341","06d92156338041bf93fa565143c7c555"]},"id":"Dr3fvjHnagws","executionInfo":{"status":"ok","timestamp":1673755178952,"user_tz":360,"elapsed":2810766,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"d105c13e-dade-4025-8f32-445c0dd9a1bc"},"id":"Dr3fvjHnagws","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 6382\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2394\n","  Number of trainable parameters = 592130\n","You're using a LongformerTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1313 to 1536 to be a multiple of `config.attention_window`: 512\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2394' max='2394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2394/2394 46:46, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Roc Auc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.691000</td>\n","      <td>0.697915</td>\n","      <td>0.503645</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.690700</td>\n","      <td>0.692556</td>\n","      <td>0.547708</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.690400</td>\n","      <td>0.691849</td>\n","      <td>0.538342</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Initializing global attention on CLS token...\n","Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1052 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1279 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1250 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1195 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1207 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2046 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1481 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1057 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1140 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 903 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1234 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1206 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1038 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2602 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 812 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1408 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1736 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1196 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1311 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1129 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 809 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1342 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1787 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1707 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1528 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 916 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2348 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1473 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 889 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 841 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 789 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1229 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1715 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1636 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1181 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1386 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1123 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1051 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1005 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 964 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2241 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 860 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1077 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 842 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1394 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1032 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1562 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1131 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1948 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 904 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1349 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2075 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 983 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 891 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 795 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1256 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1591 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2372 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1084 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 829 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 790 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 579 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2197 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1602 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1252 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 881 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1460 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2554 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1634 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1244 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1898 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1145 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 947 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1286 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1236 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 795 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1188 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 911 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2114 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1190 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 781 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1098 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1022 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1131 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1522 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1388 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1288 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 834 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 902 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1490 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2298 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1238 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 804 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1089 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1102 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 920 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1349 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2311 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 990 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 789 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1904 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 836 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1267 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1245 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1265 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1501 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1550 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1039 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2385 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 996 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 799 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1199 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1046 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1115 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2587 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2804 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 911 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 843 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 3039 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1100 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 908 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1235 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 828 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 897 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 840 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n","Saving model checkpoint to ./models/allenai/longformer-base-4096/checkpoint-500\n","Configuration saved in ./models/allenai/longformer-base-4096/checkpoint-500/config.json\n","Model weights saved in ./models/allenai/longformer-base-4096/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./models/allenai/longformer-base-4096/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./models/allenai/longformer-base-4096/checkpoint-500/special_tokens_map.json\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1188 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1220 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 873 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1601 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1219 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 804 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1620 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 847 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1096 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1276 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1736 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 728 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1148 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 802 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1209 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2075 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1370 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1420 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1237 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1035 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1130 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1525 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 858 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1409 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1304 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1201 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 966 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1445 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1063 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1091 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 718 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 859 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1083 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 999 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1558 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 870 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1232 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 887 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1064 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 842 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 824 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 734 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1227 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 868 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 839 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 907 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1340 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1060 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1106 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1594 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1335 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1431 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1588 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2656 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1547 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1075 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 874 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 815 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1409 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1230 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1129 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1407 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1852 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1264 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1390 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 916 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1171 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 910 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1134 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1836 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 770 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2217 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1530 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 784 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1165 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1347 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1369 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1048 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1101 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 848 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2002 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1839 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 844 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 864 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1294 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 795 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1281 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1904 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1622 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1055 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1085 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1637 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 985 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1708 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1126 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1685 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1322 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1413 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 821 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1244 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2179 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1201 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1353 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 870 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 885 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1699 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2044 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 939 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1728 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1678 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 689 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1124 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1328 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1490 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 513 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 813 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 958 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1161 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1178 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1373 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1165 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 939 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1210 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1862 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 822 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1076 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1111 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1211 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1189 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/9.54k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa248f6236043c8be129d5c5c29e44d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Initializing global attention on CLS token...\n","Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 718 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2372 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1736 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1134 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1195 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1388 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 999 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1236 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1904 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1736 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1634 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1311 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1304 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1390 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1591 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 819 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1089 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1707 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1340 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1550 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1286 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1620 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1370 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 795 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1115 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 889 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1558 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1064 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1131 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1602 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1102 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 579 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1207 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 873 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 809 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1100 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2804 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 579 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1232 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1106 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1063 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1098 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 887 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1501 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1715 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1525 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1221 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1057 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n","Saving model checkpoint to ./models/allenai/longformer-base-4096/checkpoint-1000\n","Configuration saved in ./models/allenai/longformer-base-4096/checkpoint-1000/config.json\n","Model weights saved in ./models/allenai/longformer-base-4096/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./models/allenai/longformer-base-4096/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./models/allenai/longformer-base-4096/checkpoint-1000/special_tokens_map.json\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1431 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1836 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 789 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 3039 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1035 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1219 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1091 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 907 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1420 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 966 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1636 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 878 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1530 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1060 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 815 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1129 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1190 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 799 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1490 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1787 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 870 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1130 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2197 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 842 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1131 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1473 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1075 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2046 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1209 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 881 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1276 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1188 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1528 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1084 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2311 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2075 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1083 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1220 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1022 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1562 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1394 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2656 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1547 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1145 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 802 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 836 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 843 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1460 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2587 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 860 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2241 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1265 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 842 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 824 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2602 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 828 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1181 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1409 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1234 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1601 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 823 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1003 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1244 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 891 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1898 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 853 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1313 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 812 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 790 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1227 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1171 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1118 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1386 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1129 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1077 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1264 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 897 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1349 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 712 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2348 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2217 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1948 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1140 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 840 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1481 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1165 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 990 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2554 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1594 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1409 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1256 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1199 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1077 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1039 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1372 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1252 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1245 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1165 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 983 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1229 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 804 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1005 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2385 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1001 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 770 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 908 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1237 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1230 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1349 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1279 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 847 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1046 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 728 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1408 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 947 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1133 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 868 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 964 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1522 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 911 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n","Saving model checkpoint to ./models/allenai/longformer-base-4096/checkpoint-1500\n","Configuration saved in ./models/allenai/longformer-base-4096/checkpoint-1500/config.json\n","Model weights saved in ./models/allenai/longformer-base-4096/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in ./models/allenai/longformer-base-4096/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in ./models/allenai/longformer-base-4096/checkpoint-1500/special_tokens_map.json\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1032 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1588 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2075 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1029 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1288 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1852 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1238 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 812 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1237 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1201 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2114 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1407 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1133 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1206 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2298 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1267 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 829 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1335 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1123 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 859 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1188 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1347 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1369 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1048 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1101 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 848 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2002 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1839 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 844 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 864 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1294 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 795 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1281 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1904 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1622 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1055 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1085 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1637 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 985 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1708 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1126 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1685 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1322 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1413 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 821 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1244 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2179 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1201 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1353 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 870 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 885 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1699 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2044 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 939 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1728 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1678 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 689 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1124 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1328 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1490 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 513 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 813 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 958 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1161 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1178 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1373 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1165 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 939 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1210 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1862 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 822 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1076 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1111 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1211 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1189 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 902 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2311 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1558 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1140 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 841 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1237 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1077 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1115 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1836 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2602 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1098 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1588 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2804 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1131 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1409 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 999 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1130 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1620 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 910 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2046 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1133 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1089 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1145 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 726 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1460 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2298 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 842 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 821 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 911 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1264 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 868 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2372 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 983 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1227 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1232 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1238 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1522 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 907 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 823 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2241 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1209 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1286 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1171 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 904 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1562 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 804 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1304 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 829 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1221 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 790 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1188 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1148 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1134 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 878 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1083 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1634 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1133 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1342 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 903 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1394 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1102 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2554 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1707 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 834 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 815 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 911 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1096 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1075 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 873 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 847 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 718 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1230 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 916 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1594 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1407 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1372 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 718 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1420 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1051 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1060 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1276 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1032 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1195 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1046 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1408 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1311 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1235 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1063 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1445 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1250 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 853 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 891 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1431 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1003 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1118 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1181 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1236 to 1536 to be a multiple of `config.attention_window`: 512\n","Saving model checkpoint to ./models/allenai/longformer-base-4096/checkpoint-2000\n","Configuration saved in ./models/allenai/longformer-base-4096/checkpoint-2000/config.json\n","Model weights saved in ./models/allenai/longformer-base-4096/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ./models/allenai/longformer-base-4096/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ./models/allenai/longformer-base-4096/checkpoint-2000/special_tokens_map.json\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1335 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1052 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1244 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1234 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1591 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 750 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 842 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 824 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 795 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 781 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1091 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1386 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1129 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1196 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 812 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1601 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 770 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2348 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 829 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1288 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1265 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2114 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 889 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2197 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 854 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 840 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 990 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 799 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 3039 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1501 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1388 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 809 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 579 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1313 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2217 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1787 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 799 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1229 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2385 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 839 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1084 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1390 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 947 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1852 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1038 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1129 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 819 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1165 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1636 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1199 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1237 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1190 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1715 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1948 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1340 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 859 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1106 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 823 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 712 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 964 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1528 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 836 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1252 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1029 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1349 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1349 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1550 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1123 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1736 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 916 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2075 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 920 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1736 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1370 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2075 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1001 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1898 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1530 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1057 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1904 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 804 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1188 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 789 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2587 to 3072 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1267 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1219 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1035 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1279 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1220 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1207 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 860 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1064 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 887 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1481 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1005 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1105 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1256 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1525 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1022 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1245 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1347 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1369 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1048 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1101 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 848 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2002 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1839 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 844 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 864 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1294 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 795 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1281 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1904 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1622 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1055 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1085 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1637 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 985 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1708 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1126 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1685 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1322 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1413 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 821 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1244 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2179 to 2560 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1201 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1353 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 870 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 885 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1699 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 2044 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 939 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1728 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1678 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 689 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1124 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1328 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1490 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 513 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 813 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 958 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1161 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1178 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1373 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1165 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 939 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1210 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1862 to 2048 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 822 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1076 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1111 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1211 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 1189 to 1536 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 512\n","Initializing global attention on CLS token...\n","Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=2394, training_loss=0.6908350970810816, metrics={'train_runtime': 2811.2491, 'train_samples_per_second': 6.81, 'train_steps_per_second': 0.852, 'total_flos': 9691309272105168.0, 'train_loss': 0.6908350970810816, 'epoch': 3.0})"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# Training DistilBERT-cased for 3 epochs\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"k94tIlc4hV7O","executionInfo":{"status":"ok","timestamp":1673721507230,"user_tz":360,"elapsed":425397,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}},"outputId":"1619e3d6-c396-4e0a-a02d-460c82618dfa"},"id":"k94tIlc4hV7O","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 6382\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2394\n","  Number of trainable parameters = 592130\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2394' max='2394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2394/2394 07:04, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Roc Auc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.694500</td>\n","      <td>0.700485</td>\n","      <td>0.500583</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.691500</td>\n","      <td>0.692174</td>\n","      <td>0.530840</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.693000</td>\n","      <td>0.691573</td>\n","      <td>0.535383</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./models/distilbert-base-cased-final/checkpoint-500\n","Configuration saved in ./models/distilbert-base-cased-final/checkpoint-500/config.json\n","Model weights saved in ./models/distilbert-base-cased-final/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased-final/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased-final/checkpoint-500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Saving model checkpoint to ./models/distilbert-base-cased-final/checkpoint-1000\n","Configuration saved in ./models/distilbert-base-cased-final/checkpoint-1000/config.json\n","Model weights saved in ./models/distilbert-base-cased-final/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased-final/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased-final/checkpoint-1000/special_tokens_map.json\n","Saving model checkpoint to ./models/distilbert-base-cased-final/checkpoint-1500\n","Configuration saved in ./models/distilbert-base-cased-final/checkpoint-1500/config.json\n","Model weights saved in ./models/distilbert-base-cased-final/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased-final/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased-final/checkpoint-1500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Saving model checkpoint to ./models/distilbert-base-cased-final/checkpoint-2000\n","Configuration saved in ./models/distilbert-base-cased-final/checkpoint-2000/config.json\n","Model weights saved in ./models/distilbert-base-cased-final/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased-final/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased-final/checkpoint-2000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=2394, training_loss=0.6928088985688505, metrics={'train_runtime': 425.0682, 'train_samples_per_second': 45.042, 'train_steps_per_second': 5.632, 'total_flos': 2439252091597704.0, 'train_loss': 0.6928088985688505, 'epoch': 3.0})"]},"metadata":{},"execution_count":182}]},{"cell_type":"code","source":["# Result of training BERT-uncased for 3 epochs\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hYdudGdTVyQI","executionInfo":{"status":"ok","timestamp":1673720277017,"user_tz":360,"elapsed":915661,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}},"outputId":"11c85ac1-86cc-477c-a303-9d4b0cd23e26"},"id":"hYdudGdTVyQI","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 6382\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2394\n","  Number of trainable parameters = 1538\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2394' max='2394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2394/2394 15:15, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Roc Auc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.700400</td>\n","      <td>0.695308</td>\n","      <td>0.509149</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.696200</td>\n","      <td>0.689593</td>\n","      <td>0.522465</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.696100</td>\n","      <td>0.688974</td>\n","      <td>0.514205</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./models/bert-base-uncased/checkpoint-500\n","Configuration saved in ./models/bert-base-uncased/checkpoint-500/config.json\n","Model weights saved in ./models/bert-base-uncased/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./models/bert-base-uncased/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./models/bert-base-uncased/checkpoint-500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Saving model checkpoint to ./models/bert-base-uncased/checkpoint-1000\n","Configuration saved in ./models/bert-base-uncased/checkpoint-1000/config.json\n","Model weights saved in ./models/bert-base-uncased/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./models/bert-base-uncased/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./models/bert-base-uncased/checkpoint-1000/special_tokens_map.json\n","Saving model checkpoint to ./models/bert-base-uncased/checkpoint-1500\n","Configuration saved in ./models/bert-base-uncased/checkpoint-1500/config.json\n","Model weights saved in ./models/bert-base-uncased/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in ./models/bert-base-uncased/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in ./models/bert-base-uncased/checkpoint-1500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Saving model checkpoint to ./models/bert-base-uncased/checkpoint-2000\n","Configuration saved in ./models/bert-base-uncased/checkpoint-2000/config.json\n","Model weights saved in ./models/bert-base-uncased/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ./models/bert-base-uncased/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ./models/bert-base-uncased/checkpoint-2000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=2394, training_loss=0.6970056391997246, metrics={'train_runtime': 915.474, 'train_samples_per_second': 20.914, 'train_steps_per_second': 2.615, 'total_flos': 4823685894566520.0, 'train_loss': 0.6970056391997246, 'epoch': 3.0})"]},"metadata":{},"execution_count":156}]},{"cell_type":"code","source":["# Note: this is the result after training DistilBERT-cased for 18 epochs\n","# (first 12 epochs not shown)\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3U_6DpodQiy_","executionInfo":{"status":"ok","timestamp":1673717132871,"user_tz":360,"elapsed":945647,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}},"outputId":"3cbd70ed-7201-4d98-e8bf-9eafafdcb929"},"id":"3U_6DpodQiy_","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 6382\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4788\n","  Number of trainable parameters = 592130\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4788' max='4788' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4788/4788 15:45, Epoch 6/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Roc Auc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.685400</td>\n","      <td>0.691872</td>\n","      <td>0.525937</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.685500</td>\n","      <td>0.691666</td>\n","      <td>0.525717</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.687700</td>\n","      <td>0.691480</td>\n","      <td>0.524914</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.686400</td>\n","      <td>0.691522</td>\n","      <td>0.524491</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.688100</td>\n","      <td>0.691648</td>\n","      <td>0.526300</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.687300</td>\n","      <td>0.691384</td>\n","      <td>0.527510</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./models/distilbert-base-cased/checkpoint-500\n","Configuration saved in ./models/distilbert-base-cased/checkpoint-500/config.json\n","Model weights saved in ./models/distilbert-base-cased/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased/checkpoint-500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Saving model checkpoint to ./models/distilbert-base-cased/checkpoint-1000\n","Configuration saved in ./models/distilbert-base-cased/checkpoint-1000/config.json\n","Model weights saved in ./models/distilbert-base-cased/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased/checkpoint-1000/special_tokens_map.json\n","Saving model checkpoint to ./models/distilbert-base-cased/checkpoint-1500\n","Configuration saved in ./models/distilbert-base-cased/checkpoint-1500/config.json\n","Model weights saved in ./models/distilbert-base-cased/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased/checkpoint-1500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Saving model checkpoint to ./models/distilbert-base-cased/checkpoint-2000\n","Configuration saved in ./models/distilbert-base-cased/checkpoint-2000/config.json\n","Model weights saved in ./models/distilbert-base-cased/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased/checkpoint-2000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Saving model checkpoint to ./models/distilbert-base-cased/checkpoint-2500\n","Configuration saved in ./models/distilbert-base-cased/checkpoint-2500/config.json\n","Model weights saved in ./models/distilbert-base-cased/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased/checkpoint-2500/special_tokens_map.json\n","Saving model checkpoint to ./models/distilbert-base-cased/checkpoint-3000\n","Configuration saved in ./models/distilbert-base-cased/checkpoint-3000/config.json\n","Model weights saved in ./models/distilbert-base-cased/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased/checkpoint-3000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Saving model checkpoint to ./models/distilbert-base-cased/checkpoint-3500\n","Configuration saved in ./models/distilbert-base-cased/checkpoint-3500/config.json\n","Model weights saved in ./models/distilbert-base-cased/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased/checkpoint-3500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","Saving model checkpoint to ./models/distilbert-base-cased/checkpoint-4000\n","Configuration saved in ./models/distilbert-base-cased/checkpoint-4000/config.json\n","Model weights saved in ./models/distilbert-base-cased/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased/checkpoint-4000/special_tokens_map.json\n","Saving model checkpoint to ./models/distilbert-base-cased/checkpoint-4500\n","Configuration saved in ./models/distilbert-base-cased/checkpoint-4500/config.json\n","Model weights saved in ./models/distilbert-base-cased/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in ./models/distilbert-base-cased/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in ./models/distilbert-base-cased/checkpoint-4500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1529\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=4788, training_loss=0.6874170422852787, metrics={'train_runtime': 945.3255, 'train_samples_per_second': 40.507, 'train_steps_per_second': 5.065, 'total_flos': 4868310920358984.0, 'train_loss': 0.6874170422852787, 'epoch': 6.0})"]},"metadata":{},"execution_count":118}]},{"cell_type":"code","source":["# BOW baseline. Code borrowed from https://vene.ro/blog/winning-arguments-attitude-change-reddit-cmv.html\n","\n","# load the data\n","import tarfile\n","import os.path\n","import json\n","import re\n","from bz2 import BZ2File\n","from urllib import request\n","from io import BytesIO\n","\n","import numpy as np\n","\n","\n","fname = \"cmv.tar.bz2\"\n","url = \"https://chenhaot.com/data/cmv/\" + fname\n","\n","# download if not exists\n","if not os.path.isfile(fname):\n","    f = BytesIO()\n","    with request.urlopen(url) as resp, open(fname, 'wb') as f_disk:\n","        data = resp.read()\n","        f_disk.write(data)  # save to disk too\n","        f.write(data)\n","        f.seek(0)\n","else:\n","    f = open(fname, 'rb')\n","\n","\n","tar = tarfile.open(fileobj=f, mode=\"r\")\n","\n","# Extract the file we are interested in\n","train_fname = \"op_task/train_op_data.jsonlist.bz2\"\n","test_fname = \"op_task/heldout_op_data.jsonlist.bz2\"\n","\n","train_bzlist = tar.extractfile(train_fname)\n","\n","# Deserialize the JSON list\n","original_posts_train = [\n","    json.loads(line.decode('utf-8'))\n","    for line in BZ2File(train_bzlist)\n","]\n","\n","test_bzlist = tar.extractfile(test_fname)\n","\n","original_posts_test = [\n","    json.loads(line.decode('utf-8'))\n","    for line in BZ2File(test_bzlist)\n","]\n","f.close()"],"metadata":{"id":"w-DlhkLPBll1"},"id":"w-DlhkLPBll1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["is_malleable = np.array([post[\"delta_label\"] for post in original_posts_train])\n","is_malleable_test = np.array([post[\"delta_label\"] for post in original_posts_test])"],"metadata":{"id":"4rKaQWJ9NHI5"},"id":"4rKaQWJ9NHI5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegressionCV\n","\n","# prepare the data in Python list form\n","op_text_train = [\n","    cleanup(original_post['selftext'])\n","    for original_post\n","    in original_posts_train\n","]\n","\n","op_text_test = [\n","    cleanup(original_post['selftext'])\n","    for original_post\n","    in original_posts_test\n","]"],"metadata":{"id":"oU-6GI4ANHrH"},"id":"oU-6GI4ANHrH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["vect = TfidfVectorizer(use_idf=False, norm='l1')\n","X_train = vect.fit_transform(op_text_train)"],"metadata":{"id":"cJvlhL8gNHtC"},"id":"cJvlhL8gNHtC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr = LogisticRegressionCV(Cs=10, class_weight='balanced',\n","                          scoring='roc_auc', solver='sag',\n","                          tol=0.001, max_iter=500,\n","                          random_state=0)"],"metadata":{"id":"DNbGy_SoBlpM"},"id":"DNbGy_SoBlpM","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7bbc7378","metadata":{"id":"7bbc7378","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673759523330,"user_tz":360,"elapsed":83813,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"07ba0a5d-957c-4b72-b0d3-a466fd133ab2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegressionCV(class_weight='balanced', max_iter=500, random_state=0,\n","                     scoring='roc_auc', solver='sag', tol=0.001)"]},"metadata":{},"execution_count":47}],"source":["lr.fit(X_train, is_malleable)"]},{"cell_type":"code","source":["# Evaluating on the heldout data\n","X_test = vect.transform(op_text_test)\n","test_roc = lr.score(X_test, is_malleable_test)\n","print(\"Test ROC AUC score: {:.3f}\".format(test_roc))"],"metadata":{"id":"lKqpMQ3BONZy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673759523332,"user_tz":360,"elapsed":25,"user":{"displayName":"Dang Nguyen","userId":"04638265967075238988"}},"outputId":"04d11cf3-66d1-4b61-81a8-76e6dfaa1f62"},"id":"lKqpMQ3BONZy","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test ROC AUC score: 0.531\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gAxTBlT4ONcT"},"id":"gAxTBlT4ONcT","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wbv3LMuSQaYW"},"id":"wbv3LMuSQaYW","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pWpJCxIyONjd"},"id":"pWpJCxIyONjd","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"9c3efaa6","metadata":{"id":"9c3efaa6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"389cffc4","metadata":{"id":"389cffc4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f4ecdf4a","metadata":{"id":"f4ecdf4a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b34cb7d9","metadata":{"id":"b34cb7d9"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"colab":{"provenance":[]},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"b90ccd6acab04aafb70f693303b208bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6135d440b6fc4ce79916263cc7a2b7f7","IPY_MODEL_d9d9ec7b6447479094357c422145acab","IPY_MODEL_da0c28464edb41268590d5347973ccfe"],"layout":"IPY_MODEL_5219fe29a6f041eead0a9ca00bcdaf2f"}},"6135d440b6fc4ce79916263cc7a2b7f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a9a10d5c813470c87864998a4c8587b","placeholder":"","style":"IPY_MODEL_3ca670cae80d4efdad1f555bdf4bd3c7","value":"Casting the dataset: 100%"}},"d9d9ec7b6447479094357c422145acab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2adcac6a12647819937e127f6dd02fb","max":11,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb2f915218864a159417e192aba8cdeb","value":11}},"da0c28464edb41268590d5347973ccfe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1294d7a0e0a40cbb09f65b5ef912612","placeholder":"","style":"IPY_MODEL_c7de5c6ab18a4b50a79b47959e5a6bf3","value":" 11/11 [00:00&lt;00:00, 211.99ba/s]"}},"5219fe29a6f041eead0a9ca00bcdaf2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a9a10d5c813470c87864998a4c8587b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ca670cae80d4efdad1f555bdf4bd3c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2adcac6a12647819937e127f6dd02fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb2f915218864a159417e192aba8cdeb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d1294d7a0e0a40cbb09f65b5ef912612":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7de5c6ab18a4b50a79b47959e5a6bf3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7852c768edb4b48b0119c0ce52a969f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ced68193f7e64b1c861aed8237e0e892","IPY_MODEL_c36b5128ea704e0a81744016490c5159","IPY_MODEL_d6144d4d11c94c41a23b7b3fc4f4b4fd"],"layout":"IPY_MODEL_e20766cbf13c46f29d3029a8181c609d"}},"ced68193f7e64b1c861aed8237e0e892":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b2169b3c9424e4986b85b6308a86a8b","placeholder":"","style":"IPY_MODEL_3017da81eabb4c8fb3bfc02f551ac685","value":"Casting the dataset: 100%"}},"c36b5128ea704e0a81744016490c5159":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93144c73ebdb45ebb9992c345583e40d","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_609d977705d2424d88c073bf3f040f4c","value":2}},"d6144d4d11c94c41a23b7b3fc4f4b4fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8b7eef2edc04811b34d5591c1aa5216","placeholder":"","style":"IPY_MODEL_10e88c968f454e2eae15a5580d7d0165","value":" 2/2 [00:00&lt;00:00, 77.65ba/s]"}},"e20766cbf13c46f29d3029a8181c609d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b2169b3c9424e4986b85b6308a86a8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3017da81eabb4c8fb3bfc02f551ac685":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93144c73ebdb45ebb9992c345583e40d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"609d977705d2424d88c073bf3f040f4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8b7eef2edc04811b34d5591c1aa5216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10e88c968f454e2eae15a5580d7d0165":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bc0a23451c745ebac0d9856b59d9133":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52960526673b42578e6fa39e35de9517","IPY_MODEL_88ec15aa87d74562a9581c6c0aeec475","IPY_MODEL_4776a12eda6c4ec2a245c98350c316bb"],"layout":"IPY_MODEL_9602c17df23d41c7be4b76e756177fd4"}},"52960526673b42578e6fa39e35de9517":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3501724fb4e4e9cb1c168ec1a460c7e","placeholder":"","style":"IPY_MODEL_8293f86fe90a4fa58ef052f6c6df4d56","value":"100%"}},"88ec15aa87d74562a9581c6c0aeec475":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_484856bfdcd84205a0b8a5eeb2475416","max":11,"min":0,"orientation":"horizontal","style":"IPY_MODEL_093b953b19f544a7a8185584435628ac","value":11}},"4776a12eda6c4ec2a245c98350c316bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78da710c6d394afe9dc52bb1dd343b10","placeholder":"","style":"IPY_MODEL_642c90ce4db84f39abb16c092c3cee24","value":" 11/11 [00:02&lt;00:00,  4.45ba/s]"}},"9602c17df23d41c7be4b76e756177fd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3501724fb4e4e9cb1c168ec1a460c7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8293f86fe90a4fa58ef052f6c6df4d56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"484856bfdcd84205a0b8a5eeb2475416":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"093b953b19f544a7a8185584435628ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78da710c6d394afe9dc52bb1dd343b10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"642c90ce4db84f39abb16c092c3cee24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f0a4bb3c0464fb7824d1a544532d792":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c7391e9a36c40e4876a9083bc84554b","IPY_MODEL_b5474b1d26e44891b510bf3b5eb97806","IPY_MODEL_e2523e8aaafc4ec8ad4e2317ad3ac873"],"layout":"IPY_MODEL_8d820b5247824eb38cdb3d673f7a2437"}},"3c7391e9a36c40e4876a9083bc84554b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_516f3fc8e94e4a958e0c9afe19dd4a25","placeholder":"","style":"IPY_MODEL_c8cbead78a7940c89492929b3aa032e6","value":"100%"}},"b5474b1d26e44891b510bf3b5eb97806":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6396c49c1b364ccebb7b95d2712d048d","max":11,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1134ba360eb84d5e80deb43fbb922e2e","value":11}},"e2523e8aaafc4ec8ad4e2317ad3ac873":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68c70d4cfa9c4f20a55eb9a37699d770","placeholder":"","style":"IPY_MODEL_f6e576d5abaf4db2aa3f4220f34cccfa","value":" 11/11 [00:05&lt;00:00,  1.80ba/s]"}},"8d820b5247824eb38cdb3d673f7a2437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"516f3fc8e94e4a958e0c9afe19dd4a25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8cbead78a7940c89492929b3aa032e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6396c49c1b364ccebb7b95d2712d048d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1134ba360eb84d5e80deb43fbb922e2e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68c70d4cfa9c4f20a55eb9a37699d770":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6e576d5abaf4db2aa3f4220f34cccfa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efa248f6236043c8be129d5c5c29e44d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c4bcf750a1b49908bf285a323ea2734","IPY_MODEL_56caba446452420c9264f4716a54db73","IPY_MODEL_91ecadb081284d5b95e488d328148c92"],"layout":"IPY_MODEL_8fd971371edf43b5bc9aa2db2290b045"}},"0c4bcf750a1b49908bf285a323ea2734":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c31541a0b5840668bcbea9924626058","placeholder":"","style":"IPY_MODEL_ef094b029e4a4413a81bd742a1187611","value":"Downloading builder script: 100%"}},"56caba446452420c9264f4716a54db73":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_42f6939f28ae4eaab08fd69bfc64b9f1","max":9541,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a59af6e781246c0a7f232bc4cfd6178","value":9541}},"91ecadb081284d5b95e488d328148c92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f20948455eee42bcae5e05ec2d24f341","placeholder":"","style":"IPY_MODEL_06d92156338041bf93fa565143c7c555","value":" 9.54k/9.54k [00:00&lt;00:00, 643kB/s]"}},"8fd971371edf43b5bc9aa2db2290b045":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c31541a0b5840668bcbea9924626058":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef094b029e4a4413a81bd742a1187611":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42f6939f28ae4eaab08fd69bfc64b9f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a59af6e781246c0a7f232bc4cfd6178":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f20948455eee42bcae5e05ec2d24f341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06d92156338041bf93fa565143c7c555":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5c3c35dccb948f3a0d24086c416956f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d827d8a32bc24e5b895000b607379428","IPY_MODEL_4050f4df5ec5498483c9dfaf3dfa624a","IPY_MODEL_84fbb76d1f2c428e9ce2146811db68cf"],"layout":"IPY_MODEL_41a0e55709694fcebe503cb6ce48a11e"}},"d827d8a32bc24e5b895000b607379428":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8ce79adaf91451fbd7633c1fb737758","placeholder":"","style":"IPY_MODEL_87fb45258d8942aa80da8766f45b1be8","value":"Downloading data files: 100%"}},"4050f4df5ec5498483c9dfaf3dfa624a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aac4c34ec4314d839cb58891ca0d22a8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec6bc2e3123346bd96a537cdbc58a354","value":2}},"84fbb76d1f2c428e9ce2146811db68cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e560d0c83e3a4a9cbba138c5f5de7eb0","placeholder":"","style":"IPY_MODEL_01581332ec1b42a69c07e75f7d5aca59","value":" 2/2 [00:00&lt;00:00, 83.30it/s]"}},"41a0e55709694fcebe503cb6ce48a11e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8ce79adaf91451fbd7633c1fb737758":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87fb45258d8942aa80da8766f45b1be8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aac4c34ec4314d839cb58891ca0d22a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec6bc2e3123346bd96a537cdbc58a354":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e560d0c83e3a4a9cbba138c5f5de7eb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01581332ec1b42a69c07e75f7d5aca59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c72dfac897494f779f6153559fdda098":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92592171fb714c04848f18579f2faf59","IPY_MODEL_81c4bc00028f499595dfc82df4a23775","IPY_MODEL_db209905e74743e9b05ac37fcbf34009"],"layout":"IPY_MODEL_ac06f40dbfbc4817abe5fe77180bff2e"}},"92592171fb714c04848f18579f2faf59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_474d98c06d7b43f484d78bac2c38afa2","placeholder":"","style":"IPY_MODEL_ac6b2d55a3034e17aba39d17bf327d5f","value":"Extracting data files: 100%"}},"81c4bc00028f499595dfc82df4a23775":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e625101bc9dc48d6a4e349be4f32e951","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b30219eb843448db802d5cd8d8297745","value":2}},"db209905e74743e9b05ac37fcbf34009":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd8181132f234d8ebb1796edaf7288f0","placeholder":"","style":"IPY_MODEL_0091d023c9864b16853a6cce9a5904a9","value":" 2/2 [00:01&lt;00:00,  1.94it/s]"}},"ac06f40dbfbc4817abe5fe77180bff2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"474d98c06d7b43f484d78bac2c38afa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac6b2d55a3034e17aba39d17bf327d5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e625101bc9dc48d6a4e349be4f32e951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b30219eb843448db802d5cd8d8297745":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd8181132f234d8ebb1796edaf7288f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0091d023c9864b16853a6cce9a5904a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0e40f2e12cb4a5e854b1ed365ea674e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1a9dd10b61d4adfae7f4b4430ffe999","IPY_MODEL_a412d4e8bcc348428ffbaac310f264aa","IPY_MODEL_aab5bec01df944a99395992f7eb6c558"],"layout":"IPY_MODEL_6f537105999349f0912a8d9ab2506872"}},"e1a9dd10b61d4adfae7f4b4430ffe999":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4623517890a544fb96c9101a919e3f5a","placeholder":"","style":"IPY_MODEL_ea6e7a3700db440aaf5ee46ed2a40c86","value":"Generating train split: "}},"a412d4e8bcc348428ffbaac310f264aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_10f28d4692ac4401889e35dbf5e04715","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dbfe662121004861bcd59fab251fced9","value":1}},"aab5bec01df944a99395992f7eb6c558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dd4083f4e5f4754a4f43bf99f030902","placeholder":"","style":"IPY_MODEL_8d7591b2cb5f4d679e3e5f886908cfa5","value":" 4513/0 [00:00&lt;00:00, 41405.95 examples/s]"}},"6f537105999349f0912a8d9ab2506872":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"4623517890a544fb96c9101a919e3f5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea6e7a3700db440aaf5ee46ed2a40c86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10f28d4692ac4401889e35dbf5e04715":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"dbfe662121004861bcd59fab251fced9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5dd4083f4e5f4754a4f43bf99f030902":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d7591b2cb5f4d679e3e5f886908cfa5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"241b3dfa48204ecb9abc7ae47c95b0b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1426a492c5d4068936a896c2584f548","IPY_MODEL_7cb87532d78148b2a05a0c1b84294e2a","IPY_MODEL_434d1b9617bd4f758e01713b8833165a"],"layout":"IPY_MODEL_b1dea5ad610b4f6f9987f74141ffa6d2"}},"d1426a492c5d4068936a896c2584f548":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7e4eb8e2c194ffdb63a90b7dcb72331","placeholder":"","style":"IPY_MODEL_02a5c45eb2df4bcfb00904cc86eeb056","value":"Generating test split: "}},"7cb87532d78148b2a05a0c1b84294e2a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_30ab6dfde96f4e7192d5e0b162b5186f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c61469dac4245b8a384e7f0d12c2294","value":1}},"434d1b9617bd4f758e01713b8833165a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_533fde3de54a402792415fc30f5a87bf","placeholder":"","style":"IPY_MODEL_ef068d7bc0e948e99ed27e042ee5b75a","value":" 0/0 [00:00&lt;?, ? examples/s]"}},"b1dea5ad610b4f6f9987f74141ffa6d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"f7e4eb8e2c194ffdb63a90b7dcb72331":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02a5c45eb2df4bcfb00904cc86eeb056":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30ab6dfde96f4e7192d5e0b162b5186f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"1c61469dac4245b8a384e7f0d12c2294":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"533fde3de54a402792415fc30f5a87bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef068d7bc0e948e99ed27e042ee5b75a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4e4641486d84e88bdb6a5cb8f05f437":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75d2d00b4b42421298f9869860e4775c","IPY_MODEL_418493b907de4c3fb1d76611ae4f5ef3","IPY_MODEL_1456f7f8dbbe43739325b921d5aae85b"],"layout":"IPY_MODEL_c98bccc58b1e436bbd0dee4fb896eaac"}},"75d2d00b4b42421298f9869860e4775c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9d134ce3f7c4cbb89083f476eb2cde4","placeholder":"","style":"IPY_MODEL_eb0a7f501b024fb5aae387410b5d6d40","value":"100%"}},"418493b907de4c3fb1d76611ae4f5ef3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8251520eefc46d582bb750f3bfd1ddf","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_865eb6660143493fb35063ee446da121","value":2}},"1456f7f8dbbe43739325b921d5aae85b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ad4b96cf0c848828c9c9c2ec982f02b","placeholder":"","style":"IPY_MODEL_960287aad97a4431bfa51102cdd14c72","value":" 2/2 [00:00&lt;00:00, 78.96it/s]"}},"c98bccc58b1e436bbd0dee4fb896eaac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9d134ce3f7c4cbb89083f476eb2cde4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb0a7f501b024fb5aae387410b5d6d40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8251520eefc46d582bb750f3bfd1ddf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"865eb6660143493fb35063ee446da121":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ad4b96cf0c848828c9c9c2ec982f02b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"960287aad97a4431bfa51102cdd14c72":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e706495f91df4ddb96aab4fa0133ccee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7fb73adcadef4f4c8433fcf519b18c4d","IPY_MODEL_63c6caf3fa9f4922a195012d4202e54c","IPY_MODEL_9486388c9948426eaf36743a0fa1bd3e"],"layout":"IPY_MODEL_8c3ad4d77a8b465a8e0d64f5e2fa99c3"}},"7fb73adcadef4f4c8433fcf519b18c4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16c8b294ab8544a6b9de1542600da123","placeholder":"","style":"IPY_MODEL_e02e75473c4345a5aedaee6253af7a96","value":"100%"}},"63c6caf3fa9f4922a195012d4202e54c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_90ad710a55fc4abbb5e1a75aa1c6490c","max":10743,"min":0,"orientation":"horizontal","style":"IPY_MODEL_316a22ed58754d30a6b6465f34b1ab83","value":10743}},"9486388c9948426eaf36743a0fa1bd3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7da06fca7e5e4c86be91199265eedd93","placeholder":"","style":"IPY_MODEL_85d6ef1251bf4e4384b480e561de944d","value":" 10743/10743 [00:01&lt;00:00, 10289.42ex/s]"}},"8c3ad4d77a8b465a8e0d64f5e2fa99c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16c8b294ab8544a6b9de1542600da123":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e02e75473c4345a5aedaee6253af7a96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90ad710a55fc4abbb5e1a75aa1c6490c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"316a22ed58754d30a6b6465f34b1ab83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7da06fca7e5e4c86be91199265eedd93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85d6ef1251bf4e4384b480e561de944d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3214f9e5f254e55a26f41d4a5c176af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7139bd6597134e93a663778fa9b1f060","IPY_MODEL_fe163c4a2141400fbf0bf7bd840e53b1","IPY_MODEL_f7ff04c58cc142269c6557236cd10a8b"],"layout":"IPY_MODEL_d98fb60651794af08f55b13f07b43c20"}},"7139bd6597134e93a663778fa9b1f060":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0871162ec734e6a8b05642d2700a424","placeholder":"","style":"IPY_MODEL_89f1d668b5b04ef6b73bd1698e3baec8","value":"100%"}},"fe163c4a2141400fbf0bf7bd840e53b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ed2e6652faf49cfa5ef8a7272ed509b","max":1529,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad520c0204e64adbaccf6ba302f0f68a","value":1529}},"f7ff04c58cc142269c6557236cd10a8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf6bb00090e6424fad31f412c670b54a","placeholder":"","style":"IPY_MODEL_b542f48b845a452483f7daf574550dc6","value":" 1529/1529 [00:00&lt;00:00, 8965.33ex/s]"}},"d98fb60651794af08f55b13f07b43c20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0871162ec734e6a8b05642d2700a424":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89f1d668b5b04ef6b73bd1698e3baec8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ed2e6652faf49cfa5ef8a7272ed509b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad520c0204e64adbaccf6ba302f0f68a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf6bb00090e6424fad31f412c670b54a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b542f48b845a452483f7daf574550dc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e037ad334f6840918a3c4bb6d65d778b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb922104f6c14908ad4aba86271acfe9","IPY_MODEL_38b131715e6443288fafa5c6394bd399","IPY_MODEL_716d64fb303c415db80cf3bdcb1728f1"],"layout":"IPY_MODEL_96c46fff73894bc2ae79b2ddcd0a199a"}},"eb922104f6c14908ad4aba86271acfe9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74e42292cacb4ae2b61ce3f1587cc21d","placeholder":"","style":"IPY_MODEL_5f6b621214214721b9b412ec18d61f71","value":"Downloading: 100%"}},"38b131715e6443288fafa5c6394bd399":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_961a6b41e7ae467a813244742c702255","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1ee6faec1ec480e960fb3c4187f660a","value":28}},"716d64fb303c415db80cf3bdcb1728f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bb93691e8824e328ac4f4c273160631","placeholder":"","style":"IPY_MODEL_1a52ff08976443bb989d2e0366b6fb9b","value":" 28.0/28.0 [00:00&lt;00:00, 896B/s]"}},"96c46fff73894bc2ae79b2ddcd0a199a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74e42292cacb4ae2b61ce3f1587cc21d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f6b621214214721b9b412ec18d61f71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"961a6b41e7ae467a813244742c702255":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1ee6faec1ec480e960fb3c4187f660a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bb93691e8824e328ac4f4c273160631":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a52ff08976443bb989d2e0366b6fb9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6d86c795c9d4bc5bc5d87e977795d89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c3413b3d3d7444549399a76eb222adf8","IPY_MODEL_c7e4a396a9204ee1956611cef0cf487c","IPY_MODEL_0fdf303a37f24233bbad02f342da034f"],"layout":"IPY_MODEL_6e9a2da1bada40279a46f33513bedc42"}},"c3413b3d3d7444549399a76eb222adf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b37a6f0abfd54cf896943bc884d37adc","placeholder":"","style":"IPY_MODEL_228501d5a6254bbca784fea68932c09b","value":"Downloading: 100%"}},"c7e4a396a9204ee1956611cef0cf487c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_023051eadca744c893c8664b77d37b3d","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ac5c84aab72444b9ac2859cf837da28","value":570}},"0fdf303a37f24233bbad02f342da034f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4acb09d9bca642feb77d48a7b45e73e9","placeholder":"","style":"IPY_MODEL_7a63269df8ca49b7831f50814a8b1a9c","value":" 570/570 [00:00&lt;00:00, 33.6kB/s]"}},"6e9a2da1bada40279a46f33513bedc42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b37a6f0abfd54cf896943bc884d37adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"228501d5a6254bbca784fea68932c09b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"023051eadca744c893c8664b77d37b3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ac5c84aab72444b9ac2859cf837da28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4acb09d9bca642feb77d48a7b45e73e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a63269df8ca49b7831f50814a8b1a9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e92039a4e0c14b839c2bc1741cb46311":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_28e3c967016b4a8e931de6b4ecf87f59","IPY_MODEL_e874010ee33c4c2b9a5fbd2d71347e86","IPY_MODEL_a145262f4566466d8e3e2cdb0c3d0d1c"],"layout":"IPY_MODEL_4f20417b73dd4a4e8277a256d373e04b"}},"28e3c967016b4a8e931de6b4ecf87f59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5f1e2e6e0f14f7e9d647d135566ec38","placeholder":"","style":"IPY_MODEL_7855cbfac1e947c5bbe7a701a4234bbd","value":"Downloading: 100%"}},"e874010ee33c4c2b9a5fbd2d71347e86":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df85577f0d814c47b8fa88d7f880f970","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0674979ef3c46ec813b9d41fafd2ef6","value":231508}},"a145262f4566466d8e3e2cdb0c3d0d1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7f5b7621b3c4d04b884290e8d1864d1","placeholder":"","style":"IPY_MODEL_61e5ddba635b44f09ee16cf803668f90","value":" 232k/232k [00:00&lt;00:00, 303kB/s]"}},"4f20417b73dd4a4e8277a256d373e04b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5f1e2e6e0f14f7e9d647d135566ec38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7855cbfac1e947c5bbe7a701a4234bbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df85577f0d814c47b8fa88d7f880f970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0674979ef3c46ec813b9d41fafd2ef6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7f5b7621b3c4d04b884290e8d1864d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61e5ddba635b44f09ee16cf803668f90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"900f70509dfd447e9b04de22e69d2012":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b87aceda68d4d43b1c8063188b15e53","IPY_MODEL_9f308c6c7ea04560affa7de80f6bc6f2","IPY_MODEL_7738b4ffde9040c48ac229e7740bc9b8"],"layout":"IPY_MODEL_5453797a2d114e18b9aab6e8a17e6d61"}},"7b87aceda68d4d43b1c8063188b15e53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d3d50c715ea4f7eb8f11a96d857a92c","placeholder":"","style":"IPY_MODEL_8255b87c99094226b3998a83e58406c7","value":"Downloading: 100%"}},"9f308c6c7ea04560affa7de80f6bc6f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15aaebb7d4ce4faa8055a562057e286a","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb145261594b4b19abccda54cfc6c735","value":466062}},"7738b4ffde9040c48ac229e7740bc9b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71f4207c750b44709072e4511ad1206a","placeholder":"","style":"IPY_MODEL_cced0ee065014877a894a438995491d6","value":" 466k/466k [00:01&lt;00:00, 529kB/s]"}},"5453797a2d114e18b9aab6e8a17e6d61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d3d50c715ea4f7eb8f11a96d857a92c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8255b87c99094226b3998a83e58406c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15aaebb7d4ce4faa8055a562057e286a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb145261594b4b19abccda54cfc6c735":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71f4207c750b44709072e4511ad1206a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cced0ee065014877a894a438995491d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30eac4e487b844f482f1271f91225dec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36c358debeb543128caa4e8deecb3a1b","IPY_MODEL_9c404c3802e248a2ae55adf406bb57d0","IPY_MODEL_0fceb9f8bd8f454b925f925e5da68383"],"layout":"IPY_MODEL_bea74e72cc4b4dcb9365b9745b8958b8"}},"36c358debeb543128caa4e8deecb3a1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7f6ac69a6e64f94b1613fa747bb77c6","placeholder":"","style":"IPY_MODEL_0d35ce99d9f24a85b234d16c67c06351","value":"100%"}},"9c404c3802e248a2ae55adf406bb57d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceaa046af24f4526a4ebce5eb3195f48","max":10743,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc5f5515c9a544c39a2fdcf2c5b209b9","value":10743}},"0fceb9f8bd8f454b925f925e5da68383":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb16f1fb0c92479dbecf046f01eee9f9","placeholder":"","style":"IPY_MODEL_67a13faf22244f9ea7f03cd94902a4ea","value":" 10743/10743 [00:20&lt;00:00, 590.40ex/s]"}},"bea74e72cc4b4dcb9365b9745b8958b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7f6ac69a6e64f94b1613fa747bb77c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d35ce99d9f24a85b234d16c67c06351":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ceaa046af24f4526a4ebce5eb3195f48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc5f5515c9a544c39a2fdcf2c5b209b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb16f1fb0c92479dbecf046f01eee9f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67a13faf22244f9ea7f03cd94902a4ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d65e53a90047456fb6c0b7314f1afa96":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_520b500188af44f283c0588fcf2a8104","IPY_MODEL_5fedee0a5f424d80b7e878ae7651d02e","IPY_MODEL_3b09e32c3f7b45eeb12c0a61d04e49bc"],"layout":"IPY_MODEL_7e44d8968ee145d28c74260bcca447d6"}},"520b500188af44f283c0588fcf2a8104":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd727d1f09d842d4b191a0e3e94c6e5c","placeholder":"","style":"IPY_MODEL_4e53fafb50df4166b23b11a4133ef8c5","value":"100%"}},"5fedee0a5f424d80b7e878ae7651d02e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fe00d2b59594d0d9809b9125a757b59","max":1529,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1286f6c312dc40899bd55bddef5ff908","value":1529}},"3b09e32c3f7b45eeb12c0a61d04e49bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_280cee7177114ef3b037e5d0277027a0","placeholder":"","style":"IPY_MODEL_1cde6d77f5a24be4b23b157237d34682","value":" 1529/1529 [00:03&lt;00:00, 582.64ex/s]"}},"7e44d8968ee145d28c74260bcca447d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd727d1f09d842d4b191a0e3e94c6e5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e53fafb50df4166b23b11a4133ef8c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fe00d2b59594d0d9809b9125a757b59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1286f6c312dc40899bd55bddef5ff908":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"280cee7177114ef3b037e5d0277027a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cde6d77f5a24be4b23b157237d34682":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5dccd037611b448cb86647738e369610":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35727692b797415aab04f31d8507f120","IPY_MODEL_c81ef4dc00d64d94b3a76b24632391f4","IPY_MODEL_ad3ac590dcc84f0eb9b4172082d4e36c"],"layout":"IPY_MODEL_627a0d98a4814a87b3587880fb4c5678"}},"35727692b797415aab04f31d8507f120":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_258fd565b7fe4b43baf9285db59708e8","placeholder":"","style":"IPY_MODEL_b06feac693ef4a24820dd76f94955f5a","value":"100%"}},"c81ef4dc00d64d94b3a76b24632391f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4168406cd7449e28bc2df81f254e0fe","max":10743,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1397f5ee7bd407c995394450c7f9d76","value":10743}},"ad3ac590dcc84f0eb9b4172082d4e36c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9466f5a718a24cbaa6856fcefbecc284","placeholder":"","style":"IPY_MODEL_8983a28a572c4369a8e9ee1d77ad18bd","value":" 10743/10743 [00:04&lt;00:00, 2147.49ex/s]"}},"627a0d98a4814a87b3587880fb4c5678":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"258fd565b7fe4b43baf9285db59708e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b06feac693ef4a24820dd76f94955f5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4168406cd7449e28bc2df81f254e0fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1397f5ee7bd407c995394450c7f9d76":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9466f5a718a24cbaa6856fcefbecc284":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8983a28a572c4369a8e9ee1d77ad18bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c33ee3b8d0de4f76a22106de3a54fc8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20df600bcbb14aa3a12bfd5f3e3f47f1","IPY_MODEL_7dd725a339b94918a4ca12753de0585d","IPY_MODEL_875e3ffe72114f38a1660994769b680f"],"layout":"IPY_MODEL_161965332bbc4ef9b20e65ec9d8bbd20"}},"20df600bcbb14aa3a12bfd5f3e3f47f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5694d97709db4e9d88a9c06c372cf695","placeholder":"","style":"IPY_MODEL_c56d77c8064948bf87bfe49a358962d7","value":"100%"}},"7dd725a339b94918a4ca12753de0585d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ecacdb6501444c798aadbb455e18e39","max":1529,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6e6f218c39843b4b326165454f2c5bc","value":1529}},"875e3ffe72114f38a1660994769b680f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a49537a2a36f4c1e832a6c97e66f218b","placeholder":"","style":"IPY_MODEL_df2eef555fa14bda91129bdc1b34b8b9","value":" 1529/1529 [00:00&lt;00:00, 2608.58ex/s]"}},"161965332bbc4ef9b20e65ec9d8bbd20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5694d97709db4e9d88a9c06c372cf695":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c56d77c8064948bf87bfe49a358962d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ecacdb6501444c798aadbb455e18e39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6e6f218c39843b4b326165454f2c5bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a49537a2a36f4c1e832a6c97e66f218b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df2eef555fa14bda91129bdc1b34b8b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}